{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "<img width=\"40%\" alt=\"Bluelight AI Logo\" href=\"https://bluelightai.com/\" src=\"https://github.com/BlueLightAI/cobalt-examples/blob/main/assets/blai-logo-light.png?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Cobalt to Pick the Best Model for your E-Commerce needs \n",
    "<a href=\"https://bluelightai.com/contact\">Give Feedback</a> | <a href=\"https://bluelightai.com/\">Our Website</a> | <a href=\"https://docs.cobalt.bluelightai.com/\">Cobalt Docs</a> | <a href=\"https://bluelightaicom.slack.com/archives/C0807BUJ4KE\">Slack Channel</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Last update:** 2024-12-11 (Created: 2024-11-15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "At [BluelightAI](https://bluelightai.com/) we are **thrilled** to help you identify the best model for your use case!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Business Context for This Notebook**: \n",
    "\n",
    "An ecommerce retailer is spending millions of dollars bringing customers to their website, obtaining inventory and optimizing their models.\n",
    "\n",
    "Here, they use BluelightAI Cobalt to compare two different prospective retrieval models on their customer product search dataset before deploying one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Model and Dataset Details**\n",
    "\n",
    "We compare the retrieval performance of an [SBERT](https://www.sbert.net/docs/sentence_transformer/pretrained_models.html) and an [E5](https://huggingface.co/intfloat/e5-base-v2) model on a popular ecommerce benchmark [dataset](https://huggingface.co/datasets/Marqo/marqo-GS-10M) from Marqo. \n",
    "\n",
    "The E5 model was fine-tuned on this dataset using Marqo's ecommerce fine-tuning [Marqtune](https://www.marqo.ai/blog/introducing-marqtune) Platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies\n",
    "\n",
    "For Setup Instructions, see the [Cobalt Docs](https://docs.cobalt.bluelightai.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install cobalt-ai\n",
    "# %pip install sentence_transformers==3.3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bmsr6uFrzc0l",
    "outputId": "1444994b-ab6e-420f-a5f2-1f80fa552fa7"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# cobalt.setup_license()\n",
    "import pandas as pd\n",
    "\n",
    "import cobalt\n",
    "from cobalt.embedding_models import SentenceTransformerEmbeddingModel\n",
    "from cobalt.lab.generate_interpretable_dataframe import get_interpretable_groups\n",
    "from cobalt.lab.neighbors import get_raw_subset_with_label\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why BluelightAI Cobalt:** \n",
    "\n",
    "The time you have to understand and fix a model‚Äôs errors is limited, expensive and hard to scale to the size of your dataset. Cobalt automates the otherwise painful step of looking for patterns in how a model is performing. We also make comparing models on your dataset easy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We identify groups of customer queries (inputs into your machine learning model) that have similar natural language using [TDA](https://www.nature.com/articles/srep01236) üë• \n",
    "\n",
    "2. We provide an easy Pandas DataFrame table so you can do model comparisons on these groups of user queries. üí°\n",
    "\n",
    "3. This helps you to do risk analysis, model improvement, and model selection so that you can deploy the best possible model for your use case  üìä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Prep\n",
    "\n",
    "For each query we simple need an evaluation or performance score from using your current or prospective model(s)\n",
    "- Common [evaluation metrics](https://weaviate.io/blog/retrieval-evaluation-metrics) for search retrieval include Precision, Recall, MRR, NDCG, etc.\n",
    "- Business evaluation scores often include add-to-cart rate, purchase rate, clickthrough rate, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we precomputed a common evaluation score called [NDCG](https://www.marqo.ai/blog/what-is-normalized-discounted-cumulative-gain-ndcg) which can evaluate a product search model before deployment \n",
    "\n",
    "ie: Before deployment onto an ecommerce retailer website, it's useful to use BluelightAI Cobalt to compare how two different models perform on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The \"Score\" per query has a best value of 1, and a worst value of 0 (NDCG metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "base_path = \"https://examples.cobalt.bluelightai.com/marqo-gs-10m/v1\"\n",
    "e5_results_file = \"training_epoch_1_ndcg_per_query.csv\"\n",
    "sbert_results_file = \"ndcg_per_query_gs_100k_training_2024-10-23_mini_lm_l6_v2.csv\"\n",
    "urlretrieve(f\"{base_path}/{e5_results_file}\", e5_results_file)\n",
    "urlretrieve(f\"{base_path}/{sbert_results_file}\", sbert_results_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_minilm_ndcg_per_query_df = pd.read_csv(sbert_results_file, index_col=0)\n",
    "sbert_minilm_ndcg_per_query_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e5_marqtune_ndcg_per_query = pd.read_csv(e5_results_file, index_col=0)\n",
    "e5_marqtune_ndcg_per_query = e5_marqtune_ndcg_per_query.drop(columns=[\"Score\"])\n",
    "e5_marqtune_ndcg_per_query.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"There are {len(sbert_minilm_ndcg_per_query_df)} queries in the Sbert_minilm dataset\"\n",
    ")\n",
    "print(f\"There are {len(e5_marqtune_ndcg_per_query)} queries in the E5_marqtune dataset\")\n",
    "print(\"The queries are the same in both datasets and the rows are aligned\")\n",
    "print(\"ie: row 2 is the query for Customizable Buttons for Men in both dataframes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without BluelightAI, current approaches analyze performance using the average on the whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_print = sbert_minilm_ndcg_per_query_df[\"ndcg_score\"].mean().round(2)\n",
    "print(f\"The SBERT model had an average NDCG score of {sbert_print} on this dataset\")\n",
    "e5_print = e5_marqtune_ndcg_per_query[\"ndcg_score\"].mean().round(2)\n",
    "print(f\"The E5 model had an average NDCG score of {e5_print} on this dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Limitations of Current Approaches***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Identifying where your model is performing poorly isn't addressed by taking an average on your whole dataset\n",
    "\n",
    "- Looking at individual queries at a time to understand and improve model performance isn't scalable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How BluelightAI Cobalt address these limitations:**\n",
    "\n",
    "1. Automatically identify problematic groups of data in your model, saving days or weeks of troubleshooting effort.\n",
    "\n",
    "2. Quickly compare models and assess the deployment risk for multiple models for your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Prep: Compare Models on the same Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine our dataframes since the queries are identical and aligned\n",
    " (ie: rows 2 is the query for Customizable Buttons for Men in both dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison_df = sbert_minilm_ndcg_per_query_df.copy()\n",
    "model_comparison_df = model_comparison_df.rename(columns={\"ndcg_score\": \"sbert_model\"})\n",
    "model_comparison_df[\"e5_model\"] = e5_marqtune_ndcg_per_query[\"ndcg_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison_df[\"Switching_to_E5_Impact\"] = (\n",
    "    model_comparison_df[\"e5_model\"] - model_comparison_df[\"sbert_model\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And Now... BluelightAI Cobalt üî•\n",
    "\n",
    "1. We find groups of user queries that have similar natural language using [TDA](https://www.nature.com/articles/srep01236) üë• üîó\n",
    "\n",
    "2. We then illuminate the performance of these groups on each of your models üîé \n",
    "\n",
    "3. This makes identifying problematic groups and comparing models quick and easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load your dataframe into a `CobaltDataset`.\n",
    "ds = cobalt.CobaltDataset(model_comparison_df)\n",
    "\n",
    "m = SentenceTransformerEmbeddingModel(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Using an embedding model (ie: we specify one for you above)\n",
    "# an embedding is made for each user query\n",
    "# You can specify GPU-acceleration here.\n",
    "# If you already have embeddings for each of your samples skip this\n",
    "# and them to add_embedding_array()\n",
    "embedding = m.embed(model_comparison_df[\"query\"].tolist(), device=\"mps\")\n",
    "\n",
    "# And add the embedding to the dataset, using the \"cosine\" similarity metric.\n",
    "ds.add_embedding_array(embedding, metric=\"cosine\", name=\"sbert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Results variable has your output table!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, workspace, keywords_per_level = get_interpretable_groups(\n",
    "    ds,\n",
    "    text_column_name=\"query\",\n",
    "    n_gram_range=\"up_to_bigrams\",\n",
    "    min_level=0,\n",
    "    max_level=20,\n",
    "    max_keywords=3,\n",
    "    return_intermediates=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = (\n",
    "    results[(results[\"level\"] == 10) & (results[\"query_count\"] > 10)]\n",
    "    .round(2)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "graph = workspace.graphs[\"New Graph\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observing the Results  üß†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \"Score\" a best value of 1, and a worst value of 0 (NDCG metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easily navigate the clustering in the dataframe: \n",
    "- Filter the results by a minimum query_count\n",
    "- Sort for largest impact!\n",
    "- Etcetera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We quickly found many groups or queries, like \"corks, wine corkscrew, bottles corks\" searches, are actually better in the sbert model!\n",
    "\n",
    "**Context:** On average on the whole dataset, the e5 model had a higher ndcg performance than the sbert model because it was fine-tuned on this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples:\n",
    "1. \"corks, wine corkscrew, bottles corks\" has 11 queries and has an average performance of 0.65 as a category of user searches in the sbert model which contrasts with a score of 0.43 on the e5 model\n",
    "\n",
    "2. \"bathtub caddy, caddy, caddy book has 11 queries and has an average performance of 0.43 as a category of user searches in the sbert model\n",
    "which contrasts with a score of 0.21 on the e5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(by=[\"Switching_to_E5_Impact\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigger Groups with the \"level\" column:\n",
    "\n",
    "- The higher values for the \"level\" column retrieve larger sized groups on your source data\n",
    "\n",
    "- Each level contains all of the unique points from the source, so combine levels with caution\n",
    "\n",
    "- Levels are a part of our clustering algorithm design to enable \"zoom\" levels on patterns in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting the Original Samples for a group\n",
    "\n",
    "- For any \"Label\" you want to understand more about, pass it and its \"level\" column below (for uniqueness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the label of interest here; note that order of keywords may vary\n",
    "see_label = \"corks, bottles corks, wine corkscrew\"\n",
    "# Make sure this matches your row of interest from the results dataframe\n",
    "level_column_for_see_label = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\n",
    "    (results[\"Label\"] == see_label) & (results[\"level\"] == level_column_for_see_label)\n",
    "].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply run the next cell to see the matching source data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = get_raw_subset_with_label(\n",
    "    coarseness=level_column_for_see_label,\n",
    "    label=see_label,\n",
    "    g=graph,\n",
    "    ds=ds,\n",
    "    keywords_per_level=keywords_per_level,\n",
    ")\n",
    "raw_data.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible Next Steps:\n",
    "- **Risk Analysis**\n",
    "1. Weigh the risk for important customer query patterns on whether the performance is satisfactory for model deployment.\n",
    "\n",
    "2. You can evaluate more prospective models with BluelightAI until your bar for minimal performance is met, or\n",
    "\n",
    "3. You can do precision improvement of the models on the queries you are concerned about.\n",
    "\n",
    "- **Curate your Training Data** \n",
    "1. Ensure that your dataset is comprehensive and representative of the real-world scenarios your model will face.\n",
    "2. Curate and improve your data for fine-tuning your model. \n",
    "\n",
    "    [Marqtune](https://www.marqo.ai/blog/introducing-marqtune) helps with fine-tuning ecommerce models. \n",
    "    \n",
    "    [BluelightAI](https://bluelightai.com/) can help you to track performance at each of your fine-tuning model checkpoints for each of your queries and their associated groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to email support@bluelightai.com for enhancements üí™ or troubleshooting üôè"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center; justify-content: space-between;\">\n",
    "    <div style:\"flex: 1; text-align: left;\">\n",
    "        <a href=\"#top\" style=\"text-decoration: none; color: inherit;\"> \n",
    "            <h3>Top of Page</h3> \n",
    "        </a>\n",
    "    </div>\n",
    "    <div style:\"flex: 1; text-align: right;\">\n",
    "        <img width=\"50%\" alt=\"Bluelight AI Logo\" href=\"https://bluelightai.com/\" src=\"https://github.com/BlueLightAI/cobalt-examples/blob/main/assets/blai-logo-light.png?raw=true\" style=\"float: right;\">\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "13Hle8QT_JokBUuouU7kPM_ah672V8s_H",
     "timestamp": 1727150382407
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
